{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys; sys.path.insert(0, '..')\n",
    "import pickle\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from paradeller.analysis import consolidate_stanzas\n",
    "from paradeller.dataprep import load_and_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading real, processed data from pickle...\n",
      "--------------------------------------------------\n",
      "DONE\n",
      "\n",
      "data            type: <class 'list'>\tlen: 212,381\n",
      "duplicates      type: <class 'dict'>\tlen: 282,571\n",
      "adj_list_words  type: <class 'dict'>\tlen: 19,314\n",
      "adj_list_ids    type: <class 'dict'>\tlen: 212,381\n"
     ]
    }
   ],
   "source": [
    "data, duplicates, adj_list_words, adj_list_ids = load_and_prep(use_pickle=True)\n",
    "\n",
    "# TO REFESH:\n",
    "# data, duplicates, adj_list_words, adj_list_ids = load_and_prep(use_pickle=False, update_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet(i):\n",
    "    \"\"\"Find tweet with given id\"\"\"\n",
    "    try:\n",
    "        return next(x for x in data if x[\"id\"] == i)\n",
    "    except StopIteration as e:\n",
    "        print(\"Error: No tweet with that ID\")\n",
    "        return None\n",
    "\n",
    "showlen = lambda data: print(f\"Length: {len(data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212381"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/found_2019-07-09-00-32.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-70980777f055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/found_2019-07-09-00-32.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mall_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/found_2019-07-09-00-32.pickle'"
     ]
    }
   ],
   "source": [
    "with open('../data/found_2019-07-09-00-32.pickle', 'rb') as f:\n",
    "    all_valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_valid = {}\n",
    "# for item in all_valid_lst:\n",
    "#     all_valid[item[0]] = item[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanzas = get_stanzas(all_valid)\n",
    "# stanzas_text = [[get_tweet(x)['text'] for x in stanza] for stanza in tqdm(stanzas)]\n",
    "\n",
    "len(stanzas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanzas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanza_sorter(stanza):\n",
    "    \"\"\"\n",
    "    Sort by interesting-ness\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- points for length ---\n",
    "    ids = set(stanza)\n",
    "    len_pts = sum((len(adj_list_ids[i]) for i in ids))\n",
    "\n",
    "    \n",
    "    # --- points for variance --- \n",
    "    lineA = adj_list_ids[stanza[0]]\n",
    "    lineB = adj_list_ids[stanza[1]]\n",
    "    lineC = adj_list_ids[stanza[2]]\n",
    "    lineD = adj_list_ids[stanza[3]]\n",
    "    \n",
    "    # diff b/w A and B\n",
    "    diff_pts = len(set(lineA) ^ set(lineB))\n",
    "    \n",
    "    # points for different start words\n",
    "    start_letters = set((x[0] for x in [lineA, lineB, lineC, lineD]))\n",
    "    start_pts = len(start_letters)\n",
    "    \n",
    "    pts = sum((\n",
    "        len_pts,\n",
    "        (diff_pts * 8),\n",
    "        (start_pts * 20)\n",
    "    ))\n",
    "    return pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_stanzas = sorted(stanzas, key=stanza_sorter, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low\n",
    "stanza_sorter(sorted_stanzas[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high\n",
    "stanza_sorter(sorted_stanzas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_stanzas = sorted_stanzas[:5]\n",
    "\n",
    "for stanza in view_stanzas:\n",
    "    print(\"~\"*50)\n",
    "    #print(stanza_sorter(stanza))\n",
    "    for i in [0,0,1,1,2,3]:\n",
    "        t = stanza[i]\n",
    "        tweet = get_tweet(t)\n",
    "        print(f\"@{tweet['author']:20} {tweet['text']} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Complete Poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, chain\n",
    "from math import factorial as fact\n",
    "\n",
    "from paradeller.analysis import find_final_stanzas_from_stanzas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanzas = get_stanzas(all_valid)\n",
    "\n",
    "# TMP\n",
    "stanzas = stanzas[:50]\n",
    "\n",
    "len(stanzas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stanzas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(stanzas)\n",
    "r = 3\n",
    "num_combos = fact(n) // (fact(r) * fact(n - r))\n",
    "num_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanzas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combos = combinations(stanzas, 3)\n",
    "combos = [c for c in all_combos if len(set().union(*c)) == 12]\n",
    "combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = find_final_stanzas_from_stanzas(stanzas, adj_list_ids, adj_list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for start_stanzas, end_stanzas in found.items():\n",
    "    print(\"~\"*50)\n",
    "    for stanza in start_stanzas:\n",
    "        for i in [0,0,1,1,2,3]:\n",
    "            t = stanza[i]\n",
    "            tweet = get_tweet(t)\n",
    "            print(f\"@{tweet['author']:20} {tweet['text']} \")\n",
    "        print(\"\")\n",
    "    for stanza in end_stanzas:\n",
    "        for line in stanza:\n",
    "            tweet = get_tweet(line)\n",
    "            print(f\"@{tweet['author']:20} {tweet['text']} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanzas[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of combos:\n",
    "\n",
    "- $n$ = types to choose from\n",
    "- $r$ = number chosen\n",
    "\n",
    "$\\frac{n!}{r!(n-r)!}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(stanzas)\n",
    "r = 3\n",
    "\n",
    "num_combos = fact(n) // (fact(r) * fact(n-r))\n",
    "num_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combos = combinations(stanzas, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered generator\n",
    "combos = (\n",
    "    c for c in all_combos\n",
    "    if len(set().union(*c)) == 12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "from paradeller.samples import load_samples\n",
    "from paradeller.helper import (\n",
    "    load_archive,\n",
    "    save_to_pickle,\n",
    "    read_from_pickle\n",
    ")\n",
    "from paradeller.dataprep import (\n",
    "    tokenize,\n",
    "    find_duplicates,\n",
    "    filter_out_duplicates,\n",
    "    filter_out_short,\n",
    "    filter_out_oddballs,\n",
    "    filter_out_oddballs_recursive,\n",
    "    restructure_data,\n",
    "    create_adj_list_by_word,\n",
    "    create_adj_list_by_id\n",
    ")\n",
    "from paradeller.analysis import (\n",
    "    find_matches,\n",
    "    find_matches_for_start_pairs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "USE_PICKLE = False\n",
    "UPDATE_PICKLE = False\n",
    "USE_SAMPLE = True\n",
    "#######################\n",
    "\n",
    "\n",
    "if USE_PICKLE:\n",
    "    print(\"Loading real, processed data from pickle...\")\n",
    "    data, duplicates, adj_list_words, adj_list_ids = read_from_pickle()\n",
    "else:\n",
    "    if USE_SAMPLE:\n",
    "        print(\"Loading unprocessed sample data...\")\n",
    "        data = load_samples()\n",
    "    else:\n",
    "        print(\"Loading unprocessed real data...\")\n",
    "        data = load_archive()\n",
    "    \n",
    "    showlen(data)\n",
    "    print(\"\\nCleaning up data...\")\n",
    "\n",
    "    # remove too short\n",
    "    print(\"> Remove too short\")\n",
    "    data = filter_out_short(data)\n",
    "    showlen(data)\n",
    "    \n",
    "    # remove duplicate phrases\n",
    "    print(\"> Remove duplicate phrases\")\n",
    "    duplicates = find_duplicates(data)\n",
    "    data = filter_out_duplicates(data, duplicates)\n",
    "    showlen(data)\n",
    "\n",
    "    # remove oddballs (too few matches)\n",
    "    print(\"> Recursively remove oddballs\")\n",
    "    data = filter_out_oddballs_recursive(data)\n",
    "    showlen(data)\n",
    "\n",
    "    print(\"\\nCreating adjacency lists...\")\n",
    "    # make adj lists\n",
    "    adj_list_words, adj_list_ids = restructure_data(data)\n",
    "    \n",
    "    if UPDATE_PICKLE:\n",
    "        print(\"\\nSaving new data to pickle...\")\n",
    "        save_to_pickle((data, duplicates, adj_list_words, adj_list_ids))\n",
    "        \n",
    "    \n",
    "print(\"-\"*50)\n",
    "print(\"DONE\\n\")\n",
    "stuff = {\n",
    "    \"data\": data, \"duplicates\": duplicates, \"adj_list_words\": adj_list_words, \"adj_list_ids\": adj_list_ids\n",
    "}\n",
    "for k, v in stuff.items():\n",
    "    print(f\"{k:15} type: {type(v)}\\tlen: {len(v):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet(i):\n",
    "    \"\"\"Find tweet with given id\"\"\"\n",
    "    try:\n",
    "        return next(x for x in data if x[\"id\"] == i)\n",
    "    except StopIteration as e:\n",
    "        print(\"Error: No tweet with that ID\")\n",
    "        return None\n",
    "\n",
    "showlen = lambda data: print(f\"Length: {len(data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(adj_list_ids.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(combinations(ids, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_valid = find_matches_for_start_pairs(pairs, adj_list_ids, adj_list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanzas = get_stanzas(all_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
